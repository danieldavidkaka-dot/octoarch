import { GoogleGenerativeAI } from '@google/generative-ai';
import { env } from '../config/env';
import { MemorySystem } from './memory';
import { ConversationManager } from './conversation';
import { Logger } from '../utils/logger';
import { octoTools } from './agent_tools';
import { MCPManager } from './mcp_manager';
import { ToolOrchestrator } from './tool_orchestrator';
import { PromptManager } from './prompt_manager'; // üöÄ NUEVO: Gestor centralizado de prompts

interface SessionData {
    manager: ConversationManager;
    lastActive: number;
}

export class IntelligenceCore {
    private static instance: IntelligenceCore | null = null;
    
    private genAI: GoogleGenerativeAI;
    private sessions: Map<string, SessionData>;
    
    private readonly SESSION_TTL_MS = 24 * 60 * 60 * 1000;
    private cachedModel: any = null;
    private lastModelUpdate: number = 0;

    private constructor() {
        this.sessions = new Map<string, SessionData>();
        this.genAI = new GoogleGenerativeAI(env.GEMINI_API_KEY);
        Logger.info(`üß† IntelligenceCore inicializado (Arquitectura Limpia & Function Calling Nativo)`);
    }

    public static getInstance(): IntelligenceCore {
        if (!IntelligenceCore.instance) {
            IntelligenceCore.instance = new IntelligenceCore();
        }
        return IntelligenceCore.instance;
    }

    private cleanStaleSessions() {
        const now = Date.now();
        for (const [key, session] of this.sessions.entries()) {
            if (now - session.lastActive > this.SESSION_TTL_MS) {
                this.sessions.delete(key);
                Logger.info(`üßπ [Garbage Collector] Sesi√≥n expirada limpiada de RAM: ${key}`);
            }
        }
    }

    private parseBase64Image(dataURI: string) {
        const split = dataURI.split(',');
        if (split.length !== 2) return null;
        return { inlineData: { data: split[1], mimeType: split[0].split(':')[1].split(';')[0] } };
    }

    private async getModel() {
        const now = Date.now();
        if (this.cachedModel && (now - this.lastModelUpdate < 5 * 60 * 1000)) {
            return this.cachedModel; 
        }

        const mcpTools = await MCPManager.getInstance().getDynamicGeminiTools();
        const allTools = [...octoTools, ...mcpTools];

        // üõ°Ô∏è FASE 0: Modelo oficial y estable (gemini-2.5-flash) + Instrucci√≥n centralizada
        this.cachedModel = this.genAI.getGenerativeModel({
            model: "gemini-2.5-flash",
            tools: allTools,
            systemInstruction: PromptManager.getSystemInstruction(),
            // üßä PARCHE ANTI-ALUCINACI√ìN
            generationConfig: {
                temperature: 0.1, // Casi cero creatividad. Respuestas anal√≠ticas y predecibles.
                topK: 32,         // Limita las palabras "raras" o fuera de contexto
                topP: 0.8         // Fomenta respuestas m√°s directas
            }
        });
        
        this.lastModelUpdate = now;
        return this.cachedModel;
    }

    private async generateWithRetry(request: any, retries = 3): Promise<any> {
        let delay = 5000;
        const model = await this.getModel();

        for (let i = 0; i < retries; i++) {
            try {
                return await model.generateContent(request);
            } catch (error: any) {
                if (error.message?.includes('429') || error.message?.includes('Quota') || error.message?.includes('503')) {
                    Logger.warn(`Rate Limit o Servidor Ocupado. Esperando ${delay}ms...`);
                    await new Promise(resolve => setTimeout(resolve, delay));
                    delay *= 2;
                } else {
                    throw error;
                }
            }
        }
        throw new Error("‚ùå Se excedi√≥ el l√≠mite de reintentos con la API de Gemini.");
    }

    async generateResponse(sessionId: string, userPrompt: string, forcedIntent: string | null = null, imageBase64: string | null = null): Promise<string> {
        try {
            this.cleanStaleSessions(); 

            if (!this.sessions.has(sessionId)) {
                this.sessions.set(sessionId, { manager: new ConversationManager(), lastActive: Date.now() });
                Logger.info(`üß† Nueva sesi√≥n cognitiva creada para: ${sessionId}`);
            }
            
            const sessionData = this.sessions.get(sessionId)!;
            sessionData.lastActive = Date.now(); 
            const activeConversation = sessionData.manager;

            const memory = await MemorySystem.recall();
            
            // üöÄ LIMPIEZA: Definimos el rol de forma pura, sin Regex obsoletos
            const activeMode = forcedIntent || 'AUTO';
            const isInvoDex = activeMode === 'INVODEX';

            if (!isInvoDex && activeConversation.needsCompression()) {
                Logger.info(`üß† Memoria a corto plazo llena [${sessionId}]. Iniciando compresi√≥n...`);
                const oldMessages = activeConversation.getMessagesToCompress().map(m => `${m.role}: ${m.content}`).join('\n');
                const summaryPrompt = `Resume brevemente esta conversaci√≥n pasada. Mant√©n datos clave. No respondas a los mensajes, solo res√∫melos:\n\n${oldMessages}`;
                
                try {
                    const summaryResult = await this.generateWithRetry({ contents: [{ role: 'user', parts: [{ text: summaryPrompt }] }] });
                    activeConversation.applyCompression(summaryResult.response.text());
                } catch (e) {
                    Logger.warn(`‚ö†Ô∏è Fall√≥ la compresi√≥n [${sessionId}], forzando recorte de seguridad.`);
                    activeConversation.applyCompression("Contexto previo omitido por l√≠mite de memoria.");
                }
            }
            
            const contents: any[] = []; 

            if (!isInvoDex) {
                activeConversation.add('user', userPrompt);
                const history = activeConversation.getHistory();
                
                let lastRole = "";
                for (const msg of history) {
                    if (!msg.content) continue;
                    const role = msg.role === 'model' ? 'model' : 'user';
                    
                    if (role === lastRole) {
                        contents[contents.length - 1].parts[0].text += `\n\n[NUEVO MENSAJE]: ${msg.content}`;
                    } else {
                        contents.push({ role, parts: [{ text: msg.content }] });
                        lastRole = role;
                    }
                }
                Logger.info(`Modo: ${activeMode} | Sesi√≥n: ${sessionId}`);
            } else {
                Logger.info(`Modo: INVODEX (Zero-Friction) | Sesi√≥n: ${sessionId}`);
            }

            // üöÄ CONSTRUCCI√ìN DEL PROMPT: Usando el nuevo PromptManager
            const currentTurnText = PromptManager.buildTurnPrompt(activeMode, memory, userPrompt, !!imageBase64);
            const currentParts: any[] = [{ text: currentTurnText }];
            
            if (imageBase64) {
                const img = this.parseBase64Image(imageBase64);
                if (img) currentParts.push(img);
            }

            if (contents.length > 0 && contents[contents.length - 1].role === 'user') {
                contents[contents.length - 1].parts.push(...currentParts);
            } else {
                contents.push({ role: 'user', parts: currentParts });
            }

            const result = await this.generateWithRetry({ contents });
            
            // Pasamos 'activeMode' en lugar de m√∫ltiples variables intent
            const finalProcessedResponse = await this.processExecution(result, activeMode, contents);

            if (!isInvoDex) {
                activeConversation.add('model', finalProcessedResponse);
            }

            return finalProcessedResponse;

        } catch (error: any) {
            Logger.error(`‚ùå Error en Core [${sessionId}]:`, error);
            return `‚ùå Error: ${error.message}`;
        }
    }

    private async processExecution(result: any, activeMode: string, conversationContext: any[]): Promise<string> {
        let toolOutputs = ""; 
        
        try {
            const functionCalls = result.response.functionCalls();
            if (!functionCalls || functionCalls.length === 0) {
                return result.response.text();
            }

            const orchestratorResult = await ToolOrchestrator.executeTurn(functionCalls, activeMode);
            toolOutputs = orchestratorResult.toolOutputs; 

            if (orchestratorResult.operationsPerformed) {
                Logger.info("üîÑ Bucle Cognitivo iniciado...");
                
                if (activeMode === 'INVODEX') {
                    Logger.info("‚ö° InvoDex: Respuesta ensamblada localmente (Bypass de LLM).");
                    return `\`\`\`json\n${orchestratorResult.extractedJson}\n\`\`\`\n\n${toolOutputs.trim()}`;
                }

                let loopPrompt = `[RESULTADOS T√âCNICOS]\n${toolOutputs}\n\n[INSTRUCCI√ìN]\nAnaliza los resultados t√©cnicos de las herramientas que acabas de usar y formula la respuesta final para el usuario. No menciones el JSON.`;
                const loopContents = [...conversationContext, { role: 'user', parts: [{ text: loopPrompt }] }];
                
                const finalResponse = await this.generateWithRetry({ contents: loopContents });
                return finalResponse.response.text();
            }

            return `**Octoarch (${activeMode}):**\nIntent√© ejecutar herramientas pero fallaron.\n\n${toolOutputs}`;

        } catch (error: any) {
            Logger.error("‚ùå Error en processExecution:", error);
            
            try { 
                const fallbackText = result.response.text(); 
                if (fallbackText && fallbackText.trim() !== "") return fallbackText;
            } catch { /* Ignoramos si falla la extracci√≥n de texto */ }
            
            if (toolOutputs.trim() !== "") {
                return `‚öôÔ∏è **Ejecuci√≥n T√©cnica (Fallback):**\n\n${toolOutputs}\n\n‚ö†Ô∏è *(El sistema complet√≥ la acci√≥n, pero hubo un corte de API al generar la respuesta).*`;
            }

            return "‚ùå Error procesando las herramientas.";
        }
    }
} 

export function getBrain(): IntelligenceCore {
    return IntelligenceCore.getInstance();
}